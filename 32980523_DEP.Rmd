---
title: "FIT5147 DEP S2 2022 - 32980523"
output:
  html_notebook:
    toc: yes
---

Project notes

-   Max 3 pages for Intro, Data Wrangling and Data Checking

-   3 pages for Data Exploration

-   Conclussion and reflection (1)

-   Bibliography - 1.

-   For data checking, write everything. Specially, consider tricky challenges that need to be considered

-   Use grammarly.

Questions for Project

-   

# FIT5147 \| Data Exploration Project

## Study of Airbnb User Preferences in Chile and Australia

> The objective is to provide an easy to follow framework for analysis that allows the analyst or PhD student to walk through their entire approach in a step-by-step manner with their colleagues or PhD supervisors.

> In each of the 6-steps, the researcher is to provide their code, outputs for the code, and any comments/thoughts that may help the reader interpret the approach that was taken.

## **1. Introduction**

For this study I've chosen two cities that I'm highly familiar with: **Santiago de Chile**, where I lived most of my life up until the age of 26; and **Melbourne, Australia** where I've lived for the past 7 years. Both of this cities are upwards of 5 million people and are visited by thousand of tourists every year.

As you will see in next section, the data sets are large enough to be studied on their own, but I see an opportunity to perform insightful research by comparing additional destinations in Australia such as the Northern Rivers region in New South Wales and Barwon South West in Victoria. Both of these areas are popular holiday destinations, and it would be interesting to contrast their characteristics to bigger cities in the study. As a result of this analysis, I hope the findings may be of use as a set of fundamental insights to both investors seeking to understand the short term rental markets, or for public policy makers to provide guidance on regulation or infrastructure decisions.

### 1.1 Research Question 1: Features of popularity on each city in the study

> What are some of the common features of popular listings in the cities included in the study? For example, can we see clusters of popularity in certain areas of the city that are near the CBD or near bars and restaurants?

#### Hypothesis

Popular listings in metropolitan cities are close to entertainment hubs (pubs and restaurants) and the CBD area. Smaller cities such as Hobart or Byron Bay present a similar pattern, but they also base their popularity on closeness to beaches and national parks.

Popular listings also rely on the type of amenities provided in the accommodation such as wi-fi, number of bathrooms, etc.

#### Potential Issues or Challenges

It's possible that each listing's popularity may be based on specific characteristics that are hard to measure with a data analysis pipeline and that are not included in the data set: design style, historical significance of the area/building, closeness to celebrity homes, etc.

### 1.2 Research Question 2: Common features of popular listings across cities in the study

> Are there any significant differences on these features across Santiago and Melbourne suburbs? For example, are the popular areas in Santiago more concentrated compared to Melbourne?

#### Hypothesis

Melbourne and Santiago are unlike each other in many respects. Melbourne's suburbs are reasonably self-contained with amenities and entertainment in walking distance, while Santiago is a more spread out city with less number of hubs per suburb. Therefore, more clusters of popularity would be expected to be seen in Santiago than Melbourne.

On the other hand, there should be evidence around popular listing characteristics that are independent of location.

#### Potential Issues or Challenges

Comparing cities that are vastly different in demographics could introduce noise into the results...

### 1.3 Research Question 3: How have the listing prices evolved during the last year across all cities in the study?

#### Hypothesis

It's expected that all listing prices have increased in the time frame of this project (2 years). In particular, the pandemic may have had a significant impact of prices on cities such as Hobart and in the Northern Rivers as people fled into towns with less restrictions. The opposite would be expected for Santiago and Melbourne.

#### Potential Issues or Challenges

some text

### 1.4 Environment Setup

The following libraries will be used for this project:

-   tidyverse

-   broom

-   ggmap

-   mapproj

-   ggplot2

-   geojsonio

```{r libraries, include=FALSE}
# for all things EDA
library(tidyverse)
# for geospatial data
library(broom)
#install.packages("geojsonio")
library(ggmap)
library(mapproj)
library(geojsonio)
library(ggmap)
# for plotting
library(ggplot2)
# other libraries (potentially)
library(janitor) # data cleaning
#install.packages("naniar")
library(naniar)
library(UpSetR)
```

------------------------------------------------------------------------

## **2. Data Collection and Measurement Strategy (Data Wrangling - Data Checking)**

### 2.1 Description of the data

The data set contains information from [Inside Airbnb](http://insideairbnb.com/get-the-data/) about four different regions:

1.  Melbourne, Australia (best city in the world)
2.  Northern Rivers, Australia (land of Byron Bay and other wonders)
3.  Tasmania, Australia (the island of the island)
4.  Santiago, Chile (hometown of the author of this report)

Furthermore, each data set contains the following individual tables:

-   **listings**: detailed listings with 74 key attributes for each listing. However, this study will mostly use are: `neighbourhood`, `latitude`, `longitude`, `room_type`, `price`, `minimum_nights`, `number_of_reviews`, `reviews_per_month`, and `availability_365.`

    -   **Total observations**: 132.572

-   **calendar**: booking price information for each listing

    -   **Total observations**:

-   **reviews:** detailed reviews for each listing.

    -   **Total observations**:

-   **GeoJSON files**:

-   Other... (TBC)

In addition, the study will use the last four available quarterly information released for each location.

### 2.2 Data Loading

The following steps will go through loading the data that will be used throughout the project.

#### 2.2.1 Listings

Although Airbnb provides a rich data set of 74 variables for listings, most of them won't be useful for the study. Therefore, I've kept only 18 variables instead. In order to do so, the complete data set for each city and time period was loaded into a data frame (132572 rows x 74 variables), but

```{r}
# combine data sets
all.listings.short <- read_csv("data/Melbourne/2022_06/listings.csv", show_col_types = FALSE)
```

NOTE: check issue with neighborhood data:

```{r}
list.files(path = "data/Santiago", recursive = TRUE, pattern="listings.csv.gz$", full.names = TRUE)
getwd()
```

```{r}
# combine data sets Santiago only
all.listings.scl <- list.files(path = "data/Santiago", recursive = TRUE, full.names = TRUE, pattern="listings.csv.gz$") %>%
  map_df(~read_csv(.,show_col_types = FALSE))
# combine data sets Melbourne only
all.listings.mel <- list.files(path = "data/Melbourne", recursive = TRUE, full.names = TRUE, pattern="listings.csv.gz$") %>%
  map_df(~read_csv(.,show_col_types = FALSE))
# combine data sets Tasmania only
all.listings.tas <- list.files(path = "data/Tasmania", recursive = TRUE, full.names = TRUE, pattern="listings.csv.gz$") %>%
  map_df(~read_csv(.,show_col_types = FALSE))
# combine data sets Northern Rivers only
all.listings.nr <- list.files(path = "data/Northern Rivers", recursive = TRUE, full.names = TRUE, pattern="listings.csv.gz$") %>%
  map_df(~read_csv(.,show_col_types = FALSE))
```

```{r}
# combine data sets
all.listings <- list.files(recursive = TRUE, pattern="listings.csv.gz$") %>%
  map_df(~read_csv(.,show_col_types = FALSE))
```

#### 2.2.2 Reviews

Intro to review information

```{r}
# combine data sets
all.reviews <- list.files(recursive = TRUE, pattern="*reviews.csv.gz") %>%
  map_df(~read_csv(.,show_col_types = FALSE))
```

Some description about the reviews in the project

#### 2.2.3 Calendar

Intro to calendar information

```{r}
# combine data sets
all.calendar <- list.files(recursive = TRUE, pattern="*calendar.csv.gz") %>%
  map_df(~read_csv(.,show_col_types = FALSE))
```

Some description about results

### 2.2.4 Neighborhoods

We will also need to load a list of neighborhoods per region as the project will be

```{r}
nb.list <- read_csv("data/neighbourhoods.csv", show_col_types = FALSE)
```

#### 2.2.4 Geo spatial Data

Loading the maps...

```{r}
# Plot it

mel.map <- geojson_read('data/Melbourne/2022_06/neighbourhoods.geojson', what="sp")

ggplot() +
  geom_polygon(data = mel.map, aes( x = long, y = lat, group = group), fill="#69b3a2", color="white") +
  theme_void() +
  coord_map()
```

### 2.3 Data Cleaning

#### Data issues (missing data, messy data, etc.)

#### Collection/Measurement

-   If data needs to be collected, what are the measurements being taken (clearly define the procedures and standardization)?

-   Are the measurements valid and reliable (is there potentially a need to add a research step here and conducting your own validity/reliability study before proceeding)?

**NOTE: there's an issue with the combination of neighborhood data**

#### Data Cleaning: Duplicates

-   What pre-processing steps were taken

First, we will check for duplicates on each data frame. The summary below represents all the summaries in the Calendar, Listings and Reviews data set.

```{r}
#some code to build a summary of duplicates
```

```{r}
# check dupes for listings
all.listings <- all.listings %>% distinct(.keep_all = TRUE)
```

```{r}
# remove duplicates
all.reviews <- all.reviews %>% distinct(.keep_all = TRUE)
```

```{r}
# remove duplicate for calendar
all.calendar <- all.calendar %>% distinct(.keep_all = TRUE)
```

```{r}
gg_miss_upset(all.listings)
## add title
```

#### Data Cleaning: Missing Values

```{r}
gg_miss_upset(all.reviews)
## add title
```

Within the `all.listings` data frame we can see that `license`, `host_neighbourhood`, `neighbourhood_group_cleansed`, `bathrooms`, and `calendar_updated` have missing values. Furthermore, this visualization tells us that all these variables have 691.141 missing values together, while smaller groupings of these variables have between 30.000 and 35.000 missing variables together.

```{r}
gg_miss_upset(all.reviews)
## add title
```

The `all.reviews` data frame has a low rate of missing values, with 800 missing values for `comments` and 1 missing value for `reviewer_name.`

```{r}
gg_miss_upset(all.calendar)
## add title
```

The `all.calendar` data frame has a fairly low rate of missing values, with only 1.432 values for `minimum_nights` `maximum_nights` combined, and only 953 missing values for `price` and `adjusted_price` combined.

Therefore the following actions were taken:

-   **listings**:

-   **reviews**:

-   **calendar**:

```{r}
# summary of missing data in listings
listings.NA <- all.listings %>% 
  miss_var_summary() %>% 
  filter(pct_miss >20)
listings.NA
listings.NA
# summary of missing data in reviews
all.reviews %>% miss_var_summary() %>% filter(pct_miss >20)
# summary of missing data in calendar
all.calendar %>% miss_var_summary() %>% filter(pct_miss >20)
```

```{r}
colSums(is.na(all.listings))
```

```{r}
listings.NA.short <- all.listings.short %>% 
  miss_var_summary() %>% 
  filter(pct_miss >20)
listings.NA.short
```

```{r}
#drop columns with higher than 20% missing values
all.listings <- all.listings %>% select(-c(listings.NA$variable))
```

(**EDIT**)Eighteen variables in the `all.listings` data frame were found to contain more than 20% of their values missing and were consequently filtered out. On the other hand, all variables were kept for `all.calendar` and `all.reviews` data frames.

We will also join the `nb.list` list of areas to the main listings data frame

```{r}
# inner join
nb.list <- read_csv("data/neighbourhoods.csv", show_col_types = FALSE)

all.listings <- all.listings %>% left_join(nb.list, by="neighbourhood")
```

Next we need to correct the data types across the dataset.

```{r}
typeof(all.listings)
```

##### Findings

-   duplicate calendar items

-   duplicate reviews

-   Clearly walk through the data cleanning process.

-   Is any data missing, if so how much?

-   Describe any imputation process for missing data.

-   If any data was removed prior to analysis explain why.

## **Step 3: Visualize & Summarize Data**

-   Once data has been collected and cleaned, provide an overview of the data using summarize statistics and visuals.

### Listings

Lets visualize the prices by city

```{r}
ggplot(data = all.listings) +
  geom_histogram(mapping = aes(x = price),)
```

Hypothesis 1:

Popular listings in metropolitan cities are close to entertainment hubs (pubs and restaurants) and the CBD area. Smaller cities such as Hobart or Byron Bay present a similar pattern, but they also base their popularity on closeness to beaches and national parks.

Popular listings also rely on the type of amenities provided in the accommodation such as wi-fi, number of bathrooms, etc.

### Calendar

### Reviews

-   Offer interpretation of visuals that may help guide the model building process or generate discussion about any underlying trends in the data specific to the research question.

## **Step 4: Model Development/Interpretation**

-   Iteratively build models (simple to complex).

-   Interpret the results of each model to explain why a more complex model or different modelling strategy may be required.

## **Step 5: Model Evaluation**

-   Evaluation the final model(s), describing model errors, model accuracy, residuals, assumptions, etc.

## **Step 6: Communication of Results**

-   Communicate the results of the final model(s) in a clear manner using visualizations and language that is understandable to the end user.

-   Explain whether or not the research question has been answered.

-   Clearly discuss any limitations of the analysis.

-   Offer suggestions for future analysis or perhaps other data sets that may be incorporated to provide a more contextual answer to the research question.

------------------------------------------------------------------------

## References

[1] [Plot polygons from GeoJSON file to leaflet map](https://www.youtube.com/watch?v=hGzlvSIhvIc) (maps)

[2] [pw2/Data-Analysis-Framework: Step-by-step template for analysis/research projects (github.com)](https://github.com/pw2/Data-Analysis-Framework/tree/master) (report format)

[3] [How to clean the datasets in R? \| R-bloggers](https://www.r-bloggers.com/2021/04/how-to-clean-the-datasets-in-r/) (data cleaning)

[4][Data Structures, Summaries, and Visualisations for Missing Data • naniar (njtierney.com)](<https://naniar.njtierney.com/>) (naniar)
